#+title: Artifact for Multi-Language Probabilistic Programming
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{inconsolata}
#+LATEX_HEADER: \usepackage[a4paper, total={6in, 8in}]{geometry}
#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \setlist[itemize,1]{leftmargin=\dimexpr 26pt-.1in}
#+OPTIONS: author:nil date:nil toc:nil
#+cite_export: biblatex numeric,backend=bibtex
#+bibliography: ~/biblio/references.bib


$$\vspace{-1in}$$
Many different probabilistic programming languages exist that specialize to
specific kinds of probabilistic programs, broadly falling into the categories of
approximate and exact inference.

This artifact for Multi-Language Probabilistic Programming provides the MultiPPL
compiler. MultiPPL is a host compiler of two syntactically and semantically
different probabilistic programming languages: an approximate language leveraging
importance sampling, and an exact language using binary decision diagrams (BDDs) for
knowledge compilation. Our work demonstrates sound interoperation of these two
languages under a Matthews and Findler-style multi-language framework[cite:@matthews2007Operational].

#+begin_export latex
  \tableofcontents
#+end_export


* Artifact Availability
The provided artifact contains the following source, development dependencies, and executables:
- This README
- The multippl source code as a separate ~multippl-source.tar.gz~ file.
- A docker image containing the following
  + executables for development: ~cargo~, ~rustc~, ~tree-sitter~, ~cargo-nextest~, ~ghc~, ~bc~
  + the multippl source code, located at ~/data/multippl-source~
  + executables for benchmarking:
    + ~multippl~ our software artifact
    + ~python~ with ~pyro~ preinstalled, our benchmark's approximate inference alternative
    + ~psi~, our benchmark's exact inference alternative
    + ~dice~, used to derive components of the ground truth.
    + ~multippl-benchmark~, a shell script which runs the benchmarks and tabulates our results.


* Quick Start
** Hardware Requirements
There are no explicit hardware requirements for the ~mulippl~ compiler. Large exact inference programs will eventually encounter memory limitations and slow down the samples produced, but this has not been an issue for networks in our evaluations.
** Running ~multippl~
The attached docker image has it's entrypoint set to the ~multippl~ binary:
#+begin_example bash
$ wget https://zenodo.org/records/<TBD>/files/multippl-docker.tar.gz
$ docker load --input multippl-docker.tar.gz
$ docker run --rm multippl:latest --steps <STEPS> --file <FILE>
#+end_example
Will invoke ~multippl~, where ~--step~ is the number of samples taken in the evaluation, and ~--file~ refers
to a valid MultiPPL program.

To run ~multippl~ on a file outside of the docker image you must bind a docker
mount to the image. Given a local file ~myexamples/beta-bernoulli.yo~, bind the mount
using ~-v~ or ~--volume <HOSTPATH>:<BINDPATH>~:
#+begin_example bash
$ docker run --rm -v $PWD/myexamples:/data/local multippl:latest \
             --file /data/local/beta-bernoulli.yo --steps 1000
0.33817887009669956
37ms
#+end_example

Notes:
- ~--rm~ removes all state from the docker container after execution
- ~--file~ can be relative to the ~/data~ directory.
- ~/data~ contains ~multippl-source/~ and ~examples/~ and binding directly to ~/data~
  (via ~-v $PWD/myexamples:/data/local~) will hide these subdirectories.
** Running ~multippl-benchmark~
To run our benchmarks single-threaded for 100 runs and cache the resulting
tables in a local ~logs/~ directory, use the following command:
#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark \
             multippl:latest all --logdir /data/logs
#+end_example

PSI will time out on 700 evaluations, which is the most time consuming portion of the benchmark. To reduce the running time, it may be prudent to limit PSI to only 10 runs, relying on the provided standard error to reflect the numbers provided:
#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark \
             multippl:latest all --logdir /data/logs --psi-runs 10
#+end_example

To do a "quick evaluation" with only 10 runs, a separate flag exists for the rest of the benchmark:
#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark \
             multippl:latest all --logdir /data/logs --num-runs 10 --psi-runs 10
#+end_example

Tables will be cached to ~logs/hybrid.rich~ and ~logs/discrete.rich~.

* MultiPPL Artifact Evaluation: Validation
The ~multippl~ compiler is responsible for providing L1 and wall-clock evaluations
for an approximate inference evaluations in Fig 11 and a discrete probabilistic
program evaluation in Fig 14.

** Hardware Requirements
There are no explicit hardware requirements for produce Fig 11 and Fig 14. These
are able to run on commercial hardware on a single thread, but a full evaluation
will take >200 hours.

Of the >200 hours ~6hrs are spent evaluating the tabulated results and 200 hours are spent waiting for 400 PSI
programs (100 runs in 4 evaluations) to reach a timeout of 30 minutes.
Parallelizing this evaluation is not advised without large amounts of RAM, as
the most expensive PSI benchmark, the ~bayesnets/alarm~ evaluation, takes up 17.2G
per thread of residential memory. Close behind ~alarm~ is the PSI
~bayesnets/insurance~ and ~grids/81~ evaluations, which uses \sim15G per thread of
residential memory. Using less RAM than this should be acceptable on a single
threaded evaluation, so long as a there is enough swap to compensate for the
difference of the expected RAM.

The ~multippl-benchmark~ tool can use more threads to speed up evaluation and to
reduce the size of the timeout, with PSI-specific flags to ensure PSI is still
run single-threaded. Parallelizing any program using exact inference may cause
programs to crash due to OOM errors. On a Thinkpad T14s Gen 3 with an AMD Ryzen
7 PRO 6850U (4.768GHz) CPU and 30G of RAM, the non-PSI portions of this
benchmark can be safely run with 8 threads.

** Running ~multippl-benchmark~ via Docker
The ~multippl-benchmark~ script is a multi-threaded benchmark evaluator, used to produce our evaluations.
To run the ~multippl-benchmark~ command, invoke
#+begin_src
$ docker run --entrypoint multippl-benchmark multippl:latest
multippl-benchmark (all|tabulate) [OPTIONS]

subcommand: all -- run all benchmarks (psi benchmarks last), then tabulate

    --num-threads NUM_THREADS Number of threads to use for non-psi benchmarks.
                              Default: 1.
    --num-runs NUM_RUNS       Number of runs to use for non-psi benchmarks.
                              Default: 100.
    --num-steps NUM_STEPS     Number of steps per run to use for non-psi,
                              approximate benchmarks. Default: 1000.

    --psi-threads PSI_THREADS Number of threads to use for psi benchmarks.
                              Default: 1.
    --psi-runs PSI_RUNS       Number of runs to use for psi benchmarks.
                              Default: 100.

    --timeout-min TIMEOUT_MIN Number of minutes before a timeout.
                              Default 30.
    --logdir LOGDIR           Directory to store execution logs.
                              Defaults to $PWD/logs.

subcommand: tabulate -- skip benchmarks and tabulate
    --logdir LOGDIR           Directory to store execution logs.
                              Defaults to $PWD/logs.
#+end_src

The default strategy is to run 100 evaluations, single-threaded, for 1000
samples.

To save the cached files locally, outside of docker, bind to a volume to the ~/data/logs~ directory:
#+begin_src bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark
             multippl:latest all
#+end_src

As stated above, PSI takes a considerable amount of time to produce the
requisite timeouts. To reduce the running time, you may reduce the timeout
duration and limit the number of runs PSI takes, relying on the provided standard error
to reflect the numbers provided:
#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark
             multippl:latest all --logdir /data/logs --psi-runs 50 --timeout-min 10
#+end_example

To speed up the non-PSI sections of the evaluation, you may increase the number of threads without parallelizing PSI processes:
#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark
             multippl:latest all --logdir /data/logs --num-threads 8
#+end_example

If the final table is not produced, the log directory should be cleared and the benchmark should be re-evaluated. Alternatively, a partial view of the table can be generated with the ~tabulate~ subcommand:

#+begin_example bash
$ docker run -v $PWD/logs:/data/logs --entrypoint multippl-benchmark
             multippl:latest tabulate --logdir /data/logs
#+end_example

** Running Benchmarks Individually
To run an individual benchmark, you must first drop into an interactive zsh or bash shell:
#+begin_example zsh
$ docker run -it --entrypoint zsh multippl:latest
#+end_example
From here, you can ~cd~ into the ~./multippl-source/bench~ folder which contains the
~bench.py~ and ~avg.py~ scripts for program execution and tabulation of a single
experiment.

Additionally, ~runall.sh~ is the source file for ~multippl-benchmark~ and
~tabulate.py~ is invoked to produce the final tables in the ~multippl-benchmark tabulate~ subcommand.

The ~bench/~ folder structure is as follows:
- ~arrival/~ contains subdirectories ~tree-15~, ~tree-31~, and ~tree-63~.
- ~bayesnets/~ contains subdirectories ~alarm~, and ~insurance~.
- ~grids/~ contains subdirectories ~3x3~, ~6x6~, and ~9x9~ corresponding to the 9, 36, and 81 evaluations in Fig 11.
- ~gossip/~ contains subdirectories ~g4~, ~g10~, and ~g20~

Each directory has a mainfile corresponding to the benchmarked tool:
- ~main.psi~ refers to the PSI program evaluated
- ~main.py~ refers to the Pyro program evaluated. When imported as a library it provides the derived groundtruth using auxiliary files ~truth.py~ or ~truth.sh~, depending on the benchmark.
- ~main.yo~ refers to a MultiPPL program with interoperation that is evaluated. We call this file ~diag.yo~ for the ~grids~ evaluations, as this specifies the collapsing strategy for interoperation.
- ~cont.yo~ refers to a MultiPPL program which only defines a Cont program.
- ~exact.yo~ refers to a MultiPPL program which only defines a Disc program.

Each experiment's subdirectory contains a symlink to ~bench.py~ in
~./multippl-source/bench/~. A benchmark is run by invoking ~python bench.py~ in the
subdirectory that generates logs in the current directory. Note that these
benchmarks default to using half of the threads visible to docker and do *not* run
PSI by default. For example:

#+begin_example zsh
$ docker run -it --entrypoint zsh multippl:latest
# in the docker shell
$ cd ./multippl-source/bench/arrival/tree-15
$ python bench.py --help
usage: bench.py [-h] [--psi] [--num-runs NUM_RUNS] [--num-steps NUM_STEPS]
                [--initial-seed INITIAL_SEED] [--noti] [--threads THREADS]
                [--logdir LOGDIR]

options:
  -h, --help            show this help message and exit
  --psi
  --timeout-min TIMEOUT_MIN
  --num-runs NUM_RUNS
  --num-steps NUM_STEPS
  --initial-seed INITIAL_SEED
  --noti
  --threads THREADS
  --logdir LOGDIR
#+end_example

Running ~bench.py~ will produce cached tables and data files in the ~$LOGDIR~.

#+print_bibliography:
